{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m DataGenerator(image_dir, mask_dir, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_gen, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[2], line 59\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     57\u001b[0m image_filenames_temp \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_filenames[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m indexes]\n\u001b[1;32m     58\u001b[0m mask_filenames_temp \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_filenames[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m indexes]\n\u001b[0;32m---> 59\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__generate_data(image_filenames_temp, mask_filenames_temp)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "Cell \u001b[0;32mIn[2], line 84\u001b[0m, in \u001b[0;36mDataGenerator.__generate_data\u001b[0;34m(self, image_filenames_temp, mask_filenames_temp)\u001b[0m\n\u001b[1;32m     81\u001b[0m     img \u001b[38;5;241m=\u001b[39m resize(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size, preserve_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m     X[i, ] \u001b[38;5;241m=\u001b[39m img\n\u001b[0;32m---> 84\u001b[0m     y[i, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(mask, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[1;32m     85\u001b[0m     y[i, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39many(mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Activation, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Custom Sobel Loss functions\n",
    "def dice_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=[1, 2, 3])\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "def unsupervised_loss(y_pred):\n",
    "    return tf.reduce_mean(tf.image.total_variation(y_pred))\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def combined_loss(y_true, y_pred):\n",
    "    supervised_mask = tf.expand_dims(y_true[..., -1], axis=-1)\n",
    "    unsupervised_mask = 1 - supervised_mask\n",
    "    y_true_masked = y_true[..., :-1] * supervised_mask\n",
    "    y_pred_masked_supervised = y_pred * supervised_mask\n",
    "    y_pred_masked_unsupervised = y_pred * unsupervised_mask\n",
    "\n",
    "    supervised_loss = dice_loss(y_true_masked, y_pred_masked_supervised)\n",
    "    unsupervised_loss_value = unsupervised_loss(y_pred_masked_unsupervised)\n",
    "\n",
    "    return supervised_loss + 0.1 * unsupervised_loss_value\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Data Generator for training\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_dir, mask_dir, batch_size=8, image_size=(256, 256), num_classes=4, shuffle=True):\n",
    "        self.image_filenames = [os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')]\n",
    "        self.mask_filenames = [os.path.join(mask_dir, x) for x in os.listdir(mask_dir) if x.endswith('.png')]\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_filenames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        image_filenames_temp = [self.image_filenames[k] for k in indexes]\n",
    "        mask_filenames_temp = [self.mask_filenames[k] for k in indexes]\n",
    "        X, y = self.__generate_data(image_filenames_temp, mask_filenames_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_filenames))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __generate_data(self, image_filenames_temp, mask_filenames_temp):\n",
    "        X = np.empty((self.batch_size, *self.image_size, 3))\n",
    "        y = np.empty((self.batch_size, *self.image_size, self.num_classes + 1))\n",
    "        for i, (img_path, mask_path) in enumerate(zip(image_filenames_temp, mask_filenames_temp)):\n",
    "            img = imread(img_path)\n",
    "            mask = imread(mask_path, as_gray=True)\n",
    "            mask = resize(mask, self.image_size, order=0, preserve_range=True)\n",
    "\n",
    "            # Ensure no unexpected values are in the mask\n",
    "            unique_values = np.unique(mask)\n",
    "            if np.any(unique_values >= self.num_classes):\n",
    "                print(f\"Unexpected values found in {mask_path}: {unique_values}\")\n",
    "                mask = np.where(mask >= self.num_classes, 0, mask)  # Remap unexpected values\n",
    "\n",
    "            if img.ndim == 2:\n",
    "                img = np.stack((img,)*3, axis=-1)\n",
    "            elif img.ndim == 3 and img.shape[-1] == 1:\n",
    "                img = np.concatenate([img]*3, axis=-1)\n",
    "\n",
    "            img = resize(img, self.image_size, preserve_range=True)\n",
    "            X[i, ] = img\n",
    "            y[i, ..., :-1] = tf.keras.utils.to_categorical(mask, num_classes=self.num_classes)\n",
    "            y[i, ..., -1] = np.any(mask > 0, axis=-1)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# U-Net Model with ResNet50 encoder\n",
    "def build_model(input_shape, num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(input_shape[0], input_shape[1], 3))\n",
    "    layer_dict = dict([(layer.name, layer.output) for layer in base_model.layers])\n",
    "    skip_connection_names = [\"conv1_relu\", \"conv2_block3_out\", \"conv3_block4_out\", \"conv4_block6_out\"]\n",
    "    skip_connections = [layer_dict[name] for name in skip_connection_names]\n",
    "    encoder_output = layer_dict[\"conv5_block3_out\"]\n",
    "\n",
    "    # Decoder\n",
    "    x = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(encoder_output)\n",
    "    x = concatenate([x, skip_connections[3]])  # Skip connection from conv4\n",
    "    x = conv_block(x, 512)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = concatenate([x, skip_connections[2]])  # Skip connection from conv3\n",
    "    x = conv_block(x, 256)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = concatenate([x, skip_connections[1]])  # Skip connection from conv2\n",
    "    x = conv_block(x, 128)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = concatenate([x, skip_connections[0]])  # Skip connection from conv1\n",
    "    x = conv_block(x, 64)\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Model Compilation and Training\n",
    "input_shape = (256, 256, 3)  # Adjusted to 3 channels\n",
    "num_classes = 4  # For three organs and background\n",
    "model = build_model(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss=combined_loss)\n",
    "\n",
    "# Define paths to your data directories\n",
    "image_dir = '/Users/arahjou/Downloads/Medical_seg/dataset_UWM_GI_Tract_train_valid/train/masks'\n",
    "mask_dir = '/Users/arahjou/Downloads/Medical_seg/dataset_UWM_GI_Tract_train_valid/train/images'\n",
    "\n",
    "train_gen = DataGenerator(image_dir, mask_dir, batch_size=8, image_size=(256, 256), num_classes=num_classes)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_gen, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting towards a more unsupervised approach for segmentation tasks usually involves leveraging techniques like autoencoders, generative adversarial networks (GANs), or self-supervised learning methods that do not rely on labeled data. However, maintaining some level of supervised learning can help guide the model, particularly in complex tasks like medical image segmentation. Here, I'll adjust the model to use a mix of self-supervised learning for feature extraction and unsupervised learning for segmentation consistency, while significantly reducing the reliance on labeled data.\n",
    "\n",
    "### Approach:\n",
    "1. **Autoencoder as a Base**: We'll use an autoencoder for learning useful representations of the data in an unsupervised manner. The encoder part will learn to compress the image data into a latent space, and the decoder will learn to reconstruct the image from this compressed representation.\n",
    "\n",
    "2. **Reconstruction Loss**: We will use a reconstruction loss which encourages the autoencoder to produce outputs as close as possible to its inputs, learning useful features in the process without needing labels.\n",
    "\n",
    "3. **Segmentation as a Side Task**: After training the autoencoder, we can attach a segmentation head to the encoded features to perform segmentation, optimizing this task with a separate unsupervised loss, such as consistency or entropy minimization over the predicted segmentation maps.\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "- **Autoencoder**: This model is trained purely unsupervised. It learns to compress and decompress the input images, capturing their essential features in the process.\n",
    "- **Data Generator**: Modified to support autoencoder training, where both inputs and outputs are the same image (self-reconstruction).\n",
    "\n",
    "### Next Steps:\n",
    "- **Segmentation as Side Task**: After training the autoencoder, explore adding a segmentation head to the encoder part and train it using an unsupervised loss (e.g., consistency loss across different augmentations of the same image).\n",
    "\n",
    "This setup emphasizes learning from unlabeled data, significantly reducing reliance on labeled data and moving closer to an unsupervised learning paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Activation, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Autoencoder for unsupervised feature learning\n",
    "def build_autoencoder(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    # Encoder\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', activation='relu')(encoded)\n",
    "    x = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    decoded = Conv2D(input_shape[2], (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "# Data Generator for training\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_dir, batch_size=8, image_size=(256, 256), shuffle=True):\n",
    "        self.image_filenames = [os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')]\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_filenames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        image_filenames_temp = [self.image_filenames[k] for k in indexes]\n",
    "        X = self.__generate_data(image_filenames_temp)\n",
    "        return X, X  # Input and output are the same for autoencoder\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_filenames))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __generate_data(self, image_filenames_temp):\n",
    "        X = np.empty((self.batch_size, *self.image_size, 3))\n",
    "        for i, img_path in enumerate(image_filenames_temp):\n",
    "            img = resize(imread(img_path), self.image_size, preserve_range=True)\n",
    "            X[i,] = img / 255.0  # Normalize images\n",
    "        return X\n",
    "\n",
    "# Train the autoencoder\n",
    "input_shape = (256, 256, 3)\n",
    "autoencoder = build_autoencoder(input_shape)\n",
    "image_dir = '/path/to/your/images'\n",
    "train_gen = DataGenerator(image_dir, batch_size=8, image_size=(256, 256))\n",
    "\n",
    "# Train autoencoder\n",
    "autoencoder.fit(train_gen, epochs=10)\n",
    "\n",
    "# Adding a segmentation head (optional)\n",
    "# This would be your task to explore how to use the learned features for segmentation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! After training the autoencoder to learn useful representations from the data, the next step involves attaching a segmentation head to the encoded features. This segmentation model will be trained to predict segmentation masks, but in this unsupervised or semi-supervised scenario, we can train it using consistency losses or pseudo-labeling approaches. Here, I'll introduce a simple unsupervised learning method for segmentation using a pseudo-labeling approach where we generate pseudo-labels based on the model's own predictions.\n",
    "\n",
    "### Segmentation Head and Pseudo-label Training\n",
    "For simplicity, we'll generate pseudo-labels from the high-confidence predictions of the segmentation model and use them as targets for further training. This approach assumes that predictions with high confidence are likely to be correct. You can further refine this by using more sophisticated consistency or entropy minimization techniques.\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Segmentation Model**: This is built by extending the encoder part of the autoencoder with a new segmentation head.\n",
    "- **Pseudo-labels**: During training, the model uses its own predictions as targets, refining these predictions iteratively.\n",
    "- **Data Generator for Segmentation**: This now also generates pseudo-labels using the segmentation model's predictions to train the model in an unsupervised manner.\n",
    "\n",
    "This setup allows you to train the segmentation model in a way that reduces reliance on labeled data, using the model's own confidence in its predictions to guide the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Activation, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build the complete model: Autoencoder + Segmentation head\n",
    "def build_segmentation_model(autoencoder, num_classes):\n",
    "    # Encoder uses the first part of the autoencoder\n",
    "    encoder = autoencoder.input\n",
    "    encoded = autoencoder.layers[-9].output  # This is an example, adjust based on your autoencoder design\n",
    "\n",
    "    # Segmentation head\n",
    "    x = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', activation='relu')(encoded)\n",
    "    x = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=encoder, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')  # Use a suitable loss function\n",
    "    return model\n",
    "\n",
    "# Generate pseudo-labels for unsupervised training\n",
    "def generate_pseudo_labels(model, data):\n",
    "    predictions = model.predict(data)\n",
    "    pseudo_labels = np.argmax(predictions, axis=-1)  # Get the class with the highest probability\n",
    "    pseudo_labels = tf.keras.utils.to_categorical(pseudo_labels, num_classes=num_classes)  # Convert to one-hot\n",
    "    return pseudo_labels\n",
    "\n",
    "# Data Generator modified for segmentation training with pseudo-labels\n",
    "class SegDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_dir, batch_size, image_size, num_classes, model=None, shuffle=True):\n",
    "        self.image_filenames = [os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')]\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.model = model\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_filenames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        image_filenames_temp = [self.image_filenames[k] for k in indexes]\n",
    "        X = self.__generate_data(image_filenames_temp)\n",
    "        if self.model:\n",
    "            y = generate_pseudo_labels(self.model, X)  # Generate pseudo-labels using the model\n",
    "        else:\n",
    "            y = np.zeros((self.batch_size, *self.image_size, self.num_classes))  # Dummy labels if model is None\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_filenames))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __generate_data(self, image_filenames_temp):\n",
    "        X = np.empty((self.batch_size, *self.image_size, 3))\n",
    "        for i, img_path in enumerate(image_filenames_temp):\n",
    "            img = resize(imread(img_path), self.image_size, preserve_range=True)\n",
    "            X[i,] = img / 255.0  # Normalize images\n",
    "        return X\n",
    "\n",
    "# Initialize and train the segmentation model with pseudo-labels\n",
    "input_shape = (256, 256, 3)\n",
    "num_classes = 4\n",
    "autoencoder = build_autoencoder(input_shape)\n",
    "segmentation_model = build_segmentation_model(autoencoder, num_classes)\n",
    "image_dir = '/path/to/your/images'\n",
    "\n",
    "# Train using the autoencoder for initial weights\n",
    "train_gen = SegDataGenerator(image_dir, batch_size=8, image_size=(256, 256), num_classes=num_classes, model=segmentation_model)\n",
    "segmentation_model.fit(train_gen, epochs=10)  # Adjust epochs as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".C_Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
