{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D_Train_Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Regular expressions for extracting parts from file paths\n",
    "GET_CASE_AND_DATE = re.compile(r\"case[0-9]{1,3}_day[0-9]{1,3}\")\n",
    "GET_SLICE_NUM = re.compile(r\"slice_[0-9]{1,4}\")\n",
    "IMG_SHAPE = re.compile(r\"_[0-9]{1,3}_[0-9]{1,3}_\")\n",
    "\n",
    "# Define classes for image segmentation\n",
    "CLASSES = [\"large_bowel\", \"small_bowel\", \"stomach\"]\n",
    "\n",
    "# Mapping from class ID to RGB color\n",
    "color2id = {(0, 0, 0): 0, (0, 0, 255): 1, (0, 255, 0): 2, (255, 0, 0): 3}\n",
    "id2color = {v: k for k, v in color2id.items()}\n",
    "\n",
    "def get_folder_files(folder_path, only_ids):\n",
    "    print(f\"Searching in folder: {folder_path}\")  # Debugging statement\n",
    "    relevant_imgs = []\n",
    "    img_ids = []\n",
    "    for dirpath, _, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            src_file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                case_day = GET_CASE_AND_DATE.search(src_file_path).group()\n",
    "                slice_id = GET_SLICE_NUM.search(src_file_path).group()\n",
    "                image_id = f\"{case_day}_{slice_id}\"\n",
    "                if image_id in only_ids:\n",
    "                    relevant_imgs.append(src_file_path)\n",
    "                    img_ids.append(image_id)\n",
    "            except AttributeError:\n",
    "                continue  # Skip files that do not match the pattern\n",
    "    print(f\"Found {len(relevant_imgs)} relevant images.\")  # Debugging statement\n",
    "    return relevant_imgs, img_ids\n",
    "\n",
    "def rgb_to_onehot_to_gray(rgb_arr, color_map=id2color):\n",
    "    num_classes = len(color_map)\n",
    "    shape = rgb_arr.shape[:2] + (num_classes,)\n",
    "    arr = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "    for i, cls in enumerate(color_map):\n",
    "        arr[:, :, i] = np.all(rgb_arr.reshape((-1, 3)) == color_map[i], axis=1).reshape(shape[:2])\n",
    "    \n",
    "    return arr.argmax(-1)\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    \"\"\"Decode run-length encoding into a binary mask.\"\"\"\n",
    "    s = np.fromstring(mask_rle, dtype=int, sep=' ')\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "def load_img(img_path):\n",
    "    \"\"\"Load and normalize an image from a file path.\"\"\"\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "    img = (img - img.min()) / (img.max() - img.min()) * 255.0\n",
    "    img = img.astype(np.uint8)\n",
    "    return np.tile(img[..., None], 3)  # Convert grayscale to RGB\n",
    "\n",
    "def create_and_write_img_msk(file_paths, file_ids, save_img_dir, save_msk_dir, main_df, mask_rgb):\n",
    "    \"\"\"Process and save images and masks from file paths.\"\"\"\n",
    "    for file_path, file_id in tqdm(zip(file_paths, file_ids), total=len(file_ids)):\n",
    "        image = load_img(file_path)\n",
    "        img_df = main_df[main_df[\"id\"] == file_id]\n",
    "        img_shape = tuple(map(int, IMG_SHAPE.search(file_path).group()[1:-1].split('_')))[::-1]\n",
    "        mask_image = np.zeros(img_shape + (3,), dtype=np.uint8)\n",
    "\n",
    "        for i, class_name in enumerate(CLASSES):\n",
    "            class_rows = img_df[img_df[\"class\"] == class_name]\n",
    "            if not class_rows.empty:\n",
    "                rle = class_rows.iloc[0]['segmentation']\n",
    "                mask_image[..., i] = rle_decode(rle, img_shape) * 255\n",
    "\n",
    "        gray_mask = rgb_to_onehot_to_gray(mask_image)\n",
    "        filename = GET_CASE_AND_DATE.search(file_path).group() + \"_\" + os.path.basename(file_path)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(save_img_dir, filename), image)\n",
    "        cv2.imwrite(os.path.join(save_msk_dir, filename), gray_mask)\n",
    "\n",
    "        if mask_rgb:\n",
    "            color_mask_dir = f\"{save_msk_dir}_rgb\"\n",
    "            os.makedirs(color_mask_dir, exist_ok=True)\n",
    "            cv2.imwrite(os.path.join(color_mask_dir, filename), mask_image)\n",
    "\n",
    "def main(csv_path, root_dir, mask_rgb=False):\n",
    "    np.random.seed(42)\n",
    "    df = pd.read_csv(csv_path).dropna()\n",
    "    ids = df['id'].unique()\n",
    "\n",
    "    orig_img_dir = os.path.join(root_dir, \"Backup\", \"train\")\n",
    "    case_folders = os.listdir(orig_img_dir)\n",
    "    \n",
    "    train_img_dir = os.path.join(root_dir, \"train\", \"images\")\n",
    "    train_msk_dir = os.path.join(root_dir, \"train\", \"masks\")\n",
    "    valid_img_dir = os.path.join(root_dir, \"valid\", \"images\")\n",
    "    valid_msk_dir = os.path.join(root_dir, \"valid\", \"masks\")\n",
    "    \n",
    "    for path in [train_img_dir, train_msk_dir, valid_img_dir, valid_msk_dir]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    for folder in case_folders:\n",
    "        folder_path = os.path.join(orig_img_dir, folder)\n",
    "        imgs, img_ids = get_folder_files(folder_path, ids)\n",
    "        if imgs:  # Check if there are images to process\n",
    "            train_imgs, valid_imgs, train_ids, valid_ids = train_test_split(imgs, img_ids, train_size=0.8, random_state=42)\n",
    "            create_and_write_img_msk(train_imgs, train_ids, train_img_dir, train_msk_dir, df, mask_rgb)\n",
    "            create_and_write_img_msk(valid_imgs, valid_ids, valid_img_dir, valid_msk_dir, df, mask_rgb)\n",
    "        else:\n",
    "            print(f\"No images to process in {folder_path}\")  # Debugging statement\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(csv_path='/Users/probio/Downloads/Medical_seg/Backup/train.csv', root_dir='/Users/probio/Downloads/Medical_seg/', mask_rgb=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D_70train_10Valid_5Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Regular expressions for extracting parts from file paths\n",
    "GET_CASE_AND_DATE = re.compile(r\"case[0-9]{1,3}_day[0-9]{1,3}\")\n",
    "GET_SLICE_NUM = re.compile(r\"slice_[0-9]{1,4}\")\n",
    "IMG_SHAPE = re.compile(r\"_[0-9]{1,3}_[0-9]{1,3}_\")\n",
    "\n",
    "# Define classes for image segmentation\n",
    "CLASSES = [\"large_bowel\", \"small_bowel\", \"stomach\"]\n",
    "\n",
    "# Mapping from class ID to RGB color\n",
    "color2id = {(0, 0, 0): 0, (0, 0, 255): 1, (0, 255, 0): 2, (255, 0, 0): 3}\n",
    "id2color = {v: k for k, v in color2id.items()}\n",
    "\n",
    "def get_folder_files(folder_path, only_ids):\n",
    "    print(f\"Searching in folder: {folder_path}\")  # Debugging statement\n",
    "    relevant_imgs = []\n",
    "    img_ids = []\n",
    "    for dirpath, _, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            src_file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                case_day = GET_CASE_AND_DATE.search(src_file_path).group()\n",
    "                slice_id = GET_SLICE_NUM.search(src_file_path).group()\n",
    "                image_id = f\"{case_day}_{slice_id}\"\n",
    "                if image_id in only_ids:\n",
    "                    relevant_imgs.append(src_file_path)\n",
    "                    img_ids.append(image_id)\n",
    "            except AttributeError:\n",
    "                continue  # Skip files that do not match the pattern\n",
    "    print(f\"Found {len(relevant_imgs)} relevant images.\")  # Debugging statement\n",
    "    return relevant_imgs, img_ids\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    \"\"\"Decode run-length encoding into a binary mask.\"\"\"\n",
    "    s = np.fromstring(mask_rle, dtype=int, sep=' ')\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "def rgb_to_onehot_to_gray(rgb_arr, color_map=id2color):\n",
    "    num_classes = len(color_map)\n",
    "    shape = rgb_arr.shape[:2] + (num_classes,)\n",
    "    arr = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "    for i, cls in enumerate(color_map):\n",
    "        arr[:, :, i] = np.all(rgb_arr.reshape((-1, 3)) == color_map[i], axis=1).reshape(shape[:2])\n",
    "    \n",
    "    return arr.argmax(-1)\n",
    "\n",
    "def load_img(img_path):\n",
    "    \"\"\"Load and normalize an image from a file path.\"\"\"\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "    img = (img - img.min()) / (img.max() - img.min()) * 255.0\n",
    "    img = img.astype(np.uint8)\n",
    "    return np.tile(img[..., None], 3)  # Convert grayscale to RGB\n",
    "\n",
    "def create_and_write_img_msk(file_paths, file_ids, save_img_dir, save_msk_dir, main_df, mask_rgb):\n",
    "    \"\"\"Process and save images and masks from file paths.\"\"\"\n",
    "    for file_path, file_id in tqdm(zip(file_paths, file_ids), total=len(file_ids)):\n",
    "        image = load_img(file_path)\n",
    "        img_df = main_df[main_df[\"id\"] == file_id]\n",
    "        img_shape = tuple(map(int, IMG_SHAPE.search(file_path).group()[1:-1].split('_')))[::-1]\n",
    "        mask_image = np.zeros(img_shape + (3,), dtype=np.uint8)\n",
    "\n",
    "        for i, class_name in enumerate(CLASSES):\n",
    "            class_rows = img_df[img_df[\"class\"] == class_name]\n",
    "            if not class_rows.empty:\n",
    "                rle = class_rows.iloc[0]['segmentation']\n",
    "                mask_image[..., i] = rle_decode(rle, img_shape) * 255\n",
    "\n",
    "        gray_mask = rgb_to_onehot_to_gray(mask_image)\n",
    "        filename = GET_CASE_AND_DATE.search(file_path).group() + \"_\" + os.path.basename(file_path)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(save_img_dir, filename), image)\n",
    "        cv2.imwrite(os.path.join(save_msk_dir, filename), gray_mask)\n",
    "\n",
    "        if mask_rgb:\n",
    "            color_mask_dir = f\"{save_msk_dir}_rgb\"\n",
    "            os.makedirs(color_mask_dir, exist_ok=True)\n",
    "            cv2.imwrite(os.path.join(color_mask_dir, filename), mask_image)\n",
    "\n",
    "def main(csv_path, root_dir, mask_rgb=False):\n",
    "    np.random.seed(42)\n",
    "    df = pd.read_csv(csv_path).dropna()\n",
    "    ids = df['id'].unique()\n",
    "\n",
    "    orig_img_dir = os.path.join(root_dir, \"Backup\", \"train\")\n",
    "    case_folders = os.listdir(orig_img_dir)\n",
    "    \n",
    "    train_img_dir = os.path.join(root_dir, \"train\", \"images\")\n",
    "    train_msk_dir = os.path.join(root_dir, \"train\", \"masks\")\n",
    "    valid_img_dir = os.path.join(root_dir, \"valid\", \"images\")\n",
    "    valid_msk_dir = os.path.join(root_dir, \"valid\", \"masks\")\n",
    "    test_img_dir = os.path.join(root_dir, \"test\", \"images\")\n",
    "    test_msk_dir = os.path.join(root_dir, \"test\", \"masks\")\n",
    "    \n",
    "    for path in [train_img_dir, train_msk_dir, valid_img_dir, valid_msk_dir, test_img_dir, test_msk_dir]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Assign train, validation, and test cases\n",
    "    train_folders, temp_folders = train_test_split(case_folders, train_size=70/85, random_state=42)\n",
    "    valid_folders, test_folders = train_test_split(temp_folders, train_size=10/15, random_state=42)\n",
    "\n",
    "    # Process each set\n",
    "    for folder_set, img_dir, msk_dir in [(train_folders, train_img_dir, train_msk_dir),\n",
    "                                         (valid_folders, valid_img_dir, valid_msk_dir),\n",
    "                                         (test_folders, test_img_dir, test_msk_dir)]:\n",
    "        for folder in folder_set:\n",
    "            folder_path = os.path.join(orig_img_dir, folder)\n",
    "            imgs, img_ids = get_folder_files(folder_path, ids)\n",
    "            if imgs:  # Check if there are images to process\n",
    "                create_and_write_img_msk(imgs, img_ids, img_dir, msk_dir, df, mask_rgb)\n",
    "            else:\n",
    "                print(f\"No images to process in {folder_path}\")  # Debugging statement\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(csv_path='/Users/probio/Downloads/Medical_seg/Backup/train.csv', root_dir='/Users/probio/Downloads/Medical_seg/', mask_rgb=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D.5_70train_10Valid_5Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacing\n",
    "\n",
    "Spacing and Depth: The spacing parameter allows you to define how far apart the slices are, while depth controls how many slices are included in each stack. Adjust these parameters to fit the specific requirements of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With unlimited stacking number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Regular expressions for extracting parts from file paths\n",
    "GET_CASE_AND_DATE = re.compile(r\"case[0-9]{1,3}_day[0-9]{1,3}\")\n",
    "GET_SLICE_NUM = re.compile(r\"slice_[0-9]{1,4}\")\n",
    "IMG_SHAPE = re.compile(r\"_[0-9]{1,3}_[0-9]{1,3}_\")\n",
    "\n",
    "# Define classes for image segmentation\n",
    "CLASSES = [\"large_bowel\", \"small_bowel\", \"stomach\"]\n",
    "\n",
    "# Mapping from class ID to RGB color\n",
    "color2id = {(0, 0, 0): 0, (0, 0, 255): 1, (0, 255, 0): 2, (255, 0, 0): 3}\n",
    "id2color = {v: k for k, v in color2id.items()}\n",
    "\n",
    "def get_folder_files(folder_path, only_ids):\n",
    "    print(f\"Searching in folder: {folder_path}\")  # Debugging statement\n",
    "    relevant_imgs = []\n",
    "    img_ids = []\n",
    "    for dirpath, _, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            src_file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                case_day = GET_CASE_AND_DATE.search(src_file_path).group()\n",
    "                slice_id = GET_SLICE_NUM.search(src_file_path).group()\n",
    "                image_id = f\"{case_day}_{slice_id}\"\n",
    "                if image_id in only_ids:\n",
    "                    relevant_imgs.append(src_file_path)\n",
    "                    img_ids.append(image_id)\n",
    "            except AttributeError:\n",
    "                continue  # Skip files that do not match the pattern\n",
    "    print(f\"Found {len(relevant_imgs)} relevant images.\")  # Debugging statement\n",
    "    return relevant_imgs, img_ids\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    \"\"\"Decode run-length encoding into a binary mask.\"\"\"\n",
    "    s = np.fromstring(mask_rle, dtype=int, sep=' ')\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "def get_slice_paths(middle_path, depth, spacing):\n",
    "    \"\"\"Generate file paths for a 2.5D image stack.\"\"\"\n",
    "    base_dir = os.path.dirname(middle_path)\n",
    "    middle_slice_index = int(GET_SLICE_NUM.search(middle_path).group().split('_')[-1])\n",
    "    slice_paths = []\n",
    "    for i in range(-(depth // 2), depth // 2 + 1):\n",
    "        if i != 0:\n",
    "            slice_num = middle_slice_index + i * spacing\n",
    "            new_path = middle_path.replace(f'slice_{middle_slice_index}', f'slice_{slice_num}')\n",
    "            if os.path.exists(new_path):\n",
    "                slice_paths.append(new_path)\n",
    "            else:\n",
    "                slice_paths.append(None)  # Handle missing slices if needed\n",
    "    return slice_paths\n",
    "\n",
    "def load_img_2p5d(middle_path, depth=3, spacing=2):\n",
    "    \"\"\"Load and stack images for 2.5D processing.\"\"\"\n",
    "    slice_paths = get_slice_paths(middle_path, depth, spacing)\n",
    "    img_shape = tuple(map(int, IMG_SHAPE.search(middle_path).group()[1:-1].split('_')))[::-1]\n",
    "    stack = np.zeros(img_shape + (depth,))\n",
    "    for i, path in enumerate(slice_paths):\n",
    "        if path and os.path.exists(path):\n",
    "            img = cv2.imread(path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "            img = (img - img.min()) / (img.max() - img.min()) * 255.0\n",
    "            stack[..., i] = img.astype(np.uint8)\n",
    "        else:\n",
    "            stack[..., i] = np.zeros(img_shape)  # Filling missing slices with zeros\n",
    "    return stack\n",
    "\n",
    "def create_and_write_img_msk_2p5d(file_paths, file_ids, save_img_dir, save_msk_dir, main_df, mask_rgb, depth, spacing):\n",
    "    \"\"\"Process and save images and masks from file paths for 2.5D images.\"\"\"\n",
    "    for file_path, file_id in tqdm(zip(file_paths, file_ids), total=len(file_ids)):\n",
    "        image_stack = load_img_2p5d(file_path, depth, spacing)\n",
    "        img_df = main_df[main_df[\"id\"] == file_id]\n",
    "        img_shape = tuple(map(int, IMG_SHAPE.search(file_path).group()[1:-1].split('_')))[::-1]\n",
    "        mask_image = np.zeros(img_shape + (3,), dtype=np.uint8)\n",
    "\n",
    "        for i, class_name in enumerate(CLASSES):\n",
    "            class_rows = img_df[img_df[\"class\"] == class_name]\n",
    "            if not class_rows.empty:\n",
    "                rle = class_rows.iloc[0]['segmentation']\n",
    "                mask_image[..., i] = rle_decode(rle, img_shape) * 255\n",
    "\n",
    "        gray_mask = rgb_to_onehot_to_gray(mask_image)\n",
    "        filename = GET_CASE_AND_DATE.search(file_path).group() + \"_\" + os.path.basename(file_path)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(save_img_dir, filename), image_stack)\n",
    "        cv2.imwrite(os.path.join(save_msk_dir, filename), gray_mask)\n",
    "\n",
    "        if mask_rgb:\n",
    "            color_mask_dir = f\"{save_msk_dir}_rgb\"\n",
    "            os.makedirs(color_mask_dir, exist_ok=True)\n",
    "            cv2.imwrite(os.path.join(color_mask_dir, filename), mask_image)\n",
    "\n",
    "def main(csv_path, root_dir, mask_rgb=False, depth=3, spacing=2):\n",
    "    np.random.seed(42)\n",
    "    df = pd.read_csv(csv_path).dropna()\n",
    "    ids = df['id'].unique()\n",
    "\n",
    "    orig_img_dir = os.path.join(root_dir, \"Backup\", \"train\")\n",
    "    case_folders = os.listdir(orig_img_dir)\n",
    "    \n",
    "    train_img_dir = os.path.join(root_dir, \"train\", \"images\")\n",
    "    train_msk_dir = os.path.join(root_dir, \"train\", \"masks\")\n",
    "    valid_img_dir = os.path.join(root_dir, \"valid\", \"images\")\n",
    "    valid_msk_dir = os.path.join(root_dir, \"valid\", \"masks\")\n",
    "    test_img_dir = os.path.join(root_dir, \"test\", \"images\")\n",
    "    test_msk_dir = os.path.join(root_dir, \"test\", \"masks\")\n",
    "    \n",
    "    for path in [train_img_dir, train_msk_dir, valid_img_dir, valid_msk_dir, test_img_dir, test_msk_dir]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Assign train, validation, and test cases\n",
    "    train_folders, temp_folders = train_test_split(case_folders, train_size=70/85, random_state=42)\n",
    "    valid_folders, test_folders = train_test_split(temp_folders, train_size=10/15, random_state=42)\n",
    "\n",
    "    # Process each set\n",
    "    for folder_set, img_dir, msk_dir in [(train_folders, train_img_dir, train_msk_dir),\n",
    "                                         (valid_folders, valid_img_dir, valid_msk_dir),\n",
    "                                         (test_folders, test_img_dir, test_msk_dir)]:\n",
    "        for folder in folder_set:\n",
    "            folder_path = os.path.join(orig_img_dir, folder)\n",
    "            imgs, img_ids = get_folder_files(folder_path, ids)\n",
    "            if imgs:  # Check if there are images to process\n",
    "                create_and_write_img_msk_2p5d(imgs, img_ids, img_dir, msk_dir, df, mask_rgb, depth, spacing)\n",
    "            else:\n",
    "                print(f\"No images to process in {folder_path}\")  # Debugging statement\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(csv_path='/Users/probio/Downloads/Medical_seg/Backup/train.csv', root_dir='/Users/probio/Downloads/Medical_seg/', mask_rgb=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in folder: /Users/probio/Downloads/Medical_seg/Backup/train/case18\n",
      "Found 287 relevant images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [00:04<00:00, 65.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in folder: /Users/probio/Downloads/Medical_seg/Backup/train/case55\n",
      "Found 159 relevant images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [00:01<00:00, 86.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in folder: /Users/probio/Downloads/Medical_seg/Backup/train/case34\n",
      "Found 132 relevant images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:01<00:00, 82.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in folder: /Users/probio/Downloads/Medical_seg/Backup/train/case7\n",
      "Found 149 relevant images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 89.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in folder: /Users/probio/Downloads/Medical_seg/Backup/train/case19\n",
      "Found 384 relevant images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 199/384 [00:02<00:02, 66.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 175\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images to process in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging statement\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 175\u001b[0m     main(csv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/probio/Downloads/Medical_seg/Backup/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/probio/Downloads/Medical_seg/\u001b[39m\u001b[38;5;124m'\u001b[39m, mask_rgb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 169\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(csv_path, root_dir, mask_rgb, depth, spacing)\u001b[0m\n\u001b[1;32m    167\u001b[0m imgs, img_ids \u001b[38;5;241m=\u001b[39m get_folder_files(folder_path, ids)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m imgs:  \u001b[38;5;66;03m# Check if there are images to process\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     create_and_write_img_msk_2p5d(imgs, img_ids, img_dir, msk_dir, df, mask_rgb, depth, spacing)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images to process in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 131\u001b[0m, in \u001b[0;36mcreate_and_write_img_msk_2p5d\u001b[0;34m(file_paths, file_ids, save_img_dir, save_msk_dir, main_df, mask_rgb, depth, spacing)\u001b[0m\n\u001b[1;32m    128\u001b[0m gray_mask \u001b[38;5;241m=\u001b[39m rgb_to_onehot_to_gray(mask_image)\n\u001b[1;32m    129\u001b[0m filename \u001b[38;5;241m=\u001b[39m GET_CASE_AND_DATE\u001b[38;5;241m.\u001b[39msearch(file_path)\u001b[38;5;241m.\u001b[39mgroup() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path)\n\u001b[0;32m--> 131\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_img_dir, filename), image_stack)\n\u001b[1;32m    132\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_msk_dir, filename), gray_mask)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask_rgb:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Regular expressions for extracting parts from file paths\n",
    "GET_CASE_AND_DATE = re.compile(r\"case[0-9]{1,3}_day[0-9]{1,3}\")\n",
    "GET_SLICE_NUM = re.compile(r\"slice_[0-9]{1,4}\")\n",
    "IMG_SHAPE = re.compile(r\"_[0-9]{1,3}_[0-9]{1,3}_\")\n",
    "\n",
    "# Define classes for image segmentation\n",
    "CLASSES = [\"large_bowel\", \"small_bowel\", \"stomach\"]\n",
    "\n",
    "# Mapping from class ID to RGB color\n",
    "color2id = {(0, 0, 0): 0, (0, 0, 255): 1, (0, 255, 0): 2, (255, 0, 0): 3}\n",
    "id2color = {v: k for k, v in color2id.items()}\n",
    "\n",
    "def get_folder_files(folder_path, only_ids):\n",
    "    print(f\"Searching in folder: {folder_path}\")  # Debugging statement\n",
    "    relevant_imgs = []\n",
    "    img_ids = []\n",
    "    for dirpath, _, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            src_file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                case_day = GET_CASE_AND_DATE.search(src_file_path).group()\n",
    "                slice_id = GET_SLICE_NUM.search(src_file_path).group()\n",
    "                image_id = f\"{case_day}_{slice_id}\"\n",
    "                if image_id in only_ids:\n",
    "                    relevant_imgs.append(src_file_path)\n",
    "                    img_ids.append(image_id)\n",
    "            except AttributeError:\n",
    "                continue  # Skip files that do not match the pattern\n",
    "    print(f\"Found {len(relevant_imgs)} relevant images.\")  # Debugging statement\n",
    "    return relevant_imgs, img_ids\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    \"\"\"Decode run-length encoding into a binary mask.\"\"\"\n",
    "    s = np.fromstring(mask_rle, dtype=int, sep=' ')\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "def rgb_to_onehot_to_gray(rgb_arr, color_map=id2color):\n",
    "    num_classes = len(color_map)\n",
    "    shape = rgb_arr.shape[:2] + (num_classes,)\n",
    "    arr = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "    for i, cls in enumerate(color_map):\n",
    "        arr[:, :, i] = np.all(rgb_arr.reshape((-1, 3)) == color_map[i], axis=1).reshape(shape[:2])\n",
    "    \n",
    "    return arr.argmax(-1)\n",
    "\n",
    "def get_slice_paths(middle_path, spacing):\n",
    "    \"\"\"Generate file paths for a 2.5D image stack.\"\"\"\n",
    "    base_dir = os.path.dirname(middle_path)\n",
    "    middle_slice_index = int(GET_SLICE_NUM.search(middle_path).group().split('_')[-1])\n",
    "    slice_paths = []\n",
    "    for i in range(-1, 2):  # -1 to 1 for 3 slices\n",
    "        slice_num = middle_slice_index + i * spacing\n",
    "        new_path = middle_path.replace(f'slice_{middle_slice_index}', f'slice_{slice_num}')\n",
    "        if os.path.exists(new_path):\n",
    "            slice_paths.append(new_path)\n",
    "        else:\n",
    "            slice_paths.append(None)  # Handle missing slices if needed\n",
    "    return slice_paths\n",
    "\n",
    "def load_img_2p5d(middle_path, spacing=2):\n",
    "    \"\"\"Load and stack images for 2.5D processing.\"\"\"\n",
    "    slice_paths = get_slice_paths(middle_path, spacing)\n",
    "    img_shape = tuple(map(int, IMG_SHAPE.search(middle_path).group()[1:-1].split('_')))[::-1]\n",
    "    stack = np.zeros(img_shape + (3,), dtype=np.uint8)  # 3 channels for RGB\n",
    "    for i, path in enumerate(slice_paths):\n",
    "        if path and os.path.exists(path):\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "            img = (img - img.min()) / (img.max() - img.min()) * 255.0\n",
    "            stack[..., i] = img.astype(np.uint8)\n",
    "        else:\n",
    "            stack[..., i] = np.zeros(img_shape, dtype=np.uint8)  # Filling missing slices with zeros\n",
    "    return stack\n",
    "\n",
    "\n",
    "\n",
    "# def load_img_2p5d(middle_path, spacing=2):\n",
    "#     slice_paths = get_slice_paths(middle_path, spacing)\n",
    "#     img_shape = tuple(map(int, IMG_SHAPE.search(middle_path).group()[1:-1].split('_')))[::-1]\n",
    "#     stack = np.zeros(img_shape + (3,), dtype=np.uint8)  # 3 channels for RGB\n",
    "\n",
    "#     print(f\"Loading slices for: {middle_path}\")\n",
    "#     for i, path in enumerate(slice_paths):\n",
    "#         if path and os.path.exists(path):\n",
    "#             img = cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "#             img = (img - img.min()) / (img.max() - img.min()) * 255.0\n",
    "#             stack[..., i] = img.astype(np.uint8)\n",
    "#             print(f\"Slice {i} stats - Min: {img.min()}, Max: {img.max()}\")  # Debug print\n",
    "#         else:\n",
    "#             stack[..., i] = np.zeros(img_shape, dtype=np.uint8)  # Filling missing slices with zeros\n",
    "#             print(f\"Slice {i} missing, filled with zeros.\")\n",
    "\n",
    "#     # Debugging: Show unique values in each channel\n",
    "#     for i in range(3):\n",
    "#         unique_values = np.unique(stack[..., i])\n",
    "#         print(f\"Unique values in channel {i}: {unique_values[:10]}... (total {len(unique_values)} unique values)\")\n",
    "    \n",
    "#     return stack\n",
    "\n",
    "\n",
    "def create_and_write_img_msk_2p5d(file_paths, file_ids, save_img_dir, save_msk_dir, main_df, mask_rgb, depth, spacing):\n",
    "    \"\"\"Process and save images and masks from file paths for 2.5D images.\"\"\"\n",
    "    for file_path, file_id in tqdm(zip(file_paths, file_ids), total=len(file_ids)):\n",
    "        image_stack = load_img_2p5d(file_path, spacing)\n",
    "        img_df = main_df[main_df[\"id\"] == file_id]\n",
    "        img_shape = tuple(map(int, IMG_SHAPE.search(file_path).group()[1:-1].split('_')))[::-1]\n",
    "        mask_image = np.zeros(img_shape + (3,), dtype=np.uint8)\n",
    "\n",
    "        for i, class_name in enumerate(CLASSES):\n",
    "            class_rows = img_df[img_df[\"class\"] == class_name]\n",
    "            if not class_rows.empty:\n",
    "                rle = class_rows.iloc[0]['segmentation']\n",
    "                mask_image[..., i] = rle_decode(rle, img_shape) * 255\n",
    "\n",
    "        gray_mask = rgb_to_onehot_to_gray(mask_image)\n",
    "        filename = GET_CASE_AND_DATE.search(file_path).group() + \"_\" + os.path.basename(file_path)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(save_img_dir, filename), image_stack)\n",
    "        cv2.imwrite(os.path.join(save_msk_dir, filename), gray_mask)\n",
    "\n",
    "        if mask_rgb:\n",
    "            color_mask_dir = f\"{save_msk_dir}_rgb\"\n",
    "            os.makedirs(color_mask_dir, exist_ok=True)\n",
    "            cv2.imwrite(os.path.join(color_mask_dir, filename), mask_image)\n",
    "\n",
    "def main(csv_path, root_dir, mask_rgb=False, depth=3, spacing=2):\n",
    "    np.random.seed(42)\n",
    "    df = pd.read_csv(csv_path).dropna()\n",
    "    ids = df['id'].unique()\n",
    "\n",
    "    orig_img_dir = os.path.join(root_dir, \"Backup\", \"train\")\n",
    "    case_folders = os.listdir(orig_img_dir)\n",
    "    \n",
    "    train_img_dir = os.path.join(root_dir, \"train\", \"images\")\n",
    "    train_msk_dir = os.path.join(root_dir, \"train\", \"masks\")\n",
    "    valid_img_dir = os.path.join(root_dir, \"valid\", \"images\")\n",
    "    valid_msk_dir = os.path.join(root_dir, \"valid\", \"masks\")\n",
    "    test_img_dir = os.path.join(root_dir, \"test\", \"images\")\n",
    "    test_msk_dir = os.path.join(root_dir, \"test\", \"masks\")\n",
    "    \n",
    "    for path in [train_img_dir, train_msk_dir, valid_img_dir, valid_msk_dir, test_img_dir, test_msk_dir]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Assign train, validation, and test cases\n",
    "    train_folders, temp_folders = train_test_split(case_folders, train_size=70/85, random_state=42)\n",
    "    valid_folders, test_folders = train_test_split(temp_folders, train_size=10/15, random_state=42)\n",
    "\n",
    "    # Process each set\n",
    "    for folder_set, img_dir, msk_dir in [(train_folders, train_img_dir, train_msk_dir),\n",
    "                                         (valid_folders, valid_img_dir, valid_msk_dir),\n",
    "                                         (test_folders, test_img_dir, test_msk_dir)]:\n",
    "        for folder in folder_set:\n",
    "            folder_path = os.path.join(orig_img_dir, folder)\n",
    "            imgs, img_ids = get_folder_files(folder_path, ids)\n",
    "            if imgs:  # Check if there are images to process\n",
    "                create_and_write_img_msk_2p5d(imgs, img_ids, img_dir, msk_dir, df, mask_rgb, depth, spacing)\n",
    "            else:\n",
    "                print(f\"No images to process in {folder_path}\")  # Debugging statement\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(csv_path='/Users/probio/Downloads/Medical_seg/Backup/train.csv', root_dir='/Users/probio/Downloads/Medical_seg/', mask_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
