{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import normalize\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_X, SIZE_Y = 288, 288\n",
    "n_classes = 4  # Adjust the number of classes based on your data\n",
    "IMG_SIZE = (SIZE_X, SIZE_Y)\n",
    "BATCH_SIZE = 16  # Reduced batch size for better memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "def multi_unet_model(n_classes=4, IMG_HEIGHT=288, IMG_WIDTH=288, IMG_CHANNELS=1):\n",
    "    # Build the model\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    # Inputs are already normalized in the preprocessing step\n",
    "    s = inputs\n",
    "\n",
    "    # Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    # Additional contraction and expansion layers as defined in your code\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "     \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "     \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "     \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    \n",
    "    #Expansive path \n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "     \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "     \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "    # Expansive path (shortened for brevity)\n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)  # Ensure the axis is correctly set for concatenation\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "    \n",
    "    # Output layer with softmax for multi-class segmentation\n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, Dropout, SeparableConv2D, Activation, BatchNormalization\n",
    "\n",
    "def multi_unet_model(n_classes=4, IMG_HEIGHT=288, IMG_WIDTH=288, IMG_CHANNELS=1):\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    \n",
    "    def conv_block(inputs, num_filters):\n",
    "        x = SeparableConv2D(num_filters, 3, padding=\"same\", \n",
    "                            depthwise_initializer=\"he_normal\",\n",
    "                            pointwise_initializer=\"he_normal\")(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = SeparableConv2D(num_filters, 3, padding=\"same\", \n",
    "                            depthwise_initializer=\"he_normal\",\n",
    "                            pointwise_initializer=\"he_normal\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "    \n",
    "    def encoder_block(inputs, num_filters):\n",
    "        x = conv_block(inputs, num_filters)\n",
    "        p = MaxPooling2D((2, 2))(x)\n",
    "        p = Dropout(0.1)(p)\n",
    "        return x, p\n",
    "    \n",
    "    def decoder_block(inputs, skip_features, num_filters):\n",
    "        x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
    "        x = concatenate([x, skip_features])\n",
    "        x = conv_block(x, num_filters)\n",
    "        return x\n",
    "\n",
    "    # Contraction path\n",
    "    c1, p1 = encoder_block(inputs, 16)\n",
    "    c2, p2 = encoder_block(p1, 32)\n",
    "    c3, p3 = encoder_block(p2, 64)\n",
    "    c4, p4 = encoder_block(p3, 128)\n",
    "    c5 = conv_block(p4, 256)\n",
    "\n",
    "    # Expansive path\n",
    "    u6 = decoder_block(c5, c4, 128)\n",
    "    u7 = decoder_block(u6, c3, 64)\n",
    "    u8 = decoder_block(u7, c2, 32)\n",
    "    u9 = decoder_block(u8, c1, 16)\n",
    "\n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(u9)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 679ms/step - Accuracy: 0.8165 - loss: 0.6677 - val_Accuracy: 0.9619 - val_loss: 0.1520\n",
      "Epoch 2/3\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 673ms/step - Accuracy: 0.9712 - loss: 0.0892 - val_Accuracy: 0.9772 - val_loss: 0.0686\n",
      "Epoch 3/3\n",
      "\u001b[1m662/662\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 1s/step - Accuracy: 0.9795 - loss: 0.0604 - val_Accuracy: 0.9808 - val_loss: 0.0581\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and required components\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, Dropout\n",
    "\n",
    "# Define paths and constants\n",
    "train_image_dir = \"/Users/arahjou/Downloads/dataset_UWM_GI_Tract_train_valid/train/images/*.png\"\n",
    "train_mask_dir = \"/Users/arahjou/Downloads/dataset_UWM_GI_Tract_train_valid/train/masks/*.png\"\n",
    "IMG_SIZE = (288, 288)  # Assuming this is your desired image size\n",
    "BATCH_SIZE = 16\n",
    "n_classes = 4  # Number of classes for segmentation\n",
    "\n",
    "# Load and preprocess images and masks\n",
    "def load_image_and_mask(image_path, mask_path):\n",
    "    def decode_img(img_path):\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_png(img, channels=1)\n",
    "        return tf.image.resize(img, IMG_SIZE)\n",
    "\n",
    "    image = decode_img(image_path)\n",
    "    mask = decode_img(mask_path)\n",
    "    mask = tf.cast(mask, tf.int8)\n",
    "    mask = tf.one_hot(tf.squeeze(mask, axis=-1), depth=n_classes)\n",
    "    return image, mask\n",
    "\n",
    "# Function to normalize images and expand dimensions\n",
    "def preprocess(image, mask):\n",
    "    image = tf.cast(image, tf.float16) / 255.0\n",
    "    return image, mask\n",
    "\n",
    "# Dataset creation\n",
    "image_paths = tf.data.Dataset.list_files(train_image_dir, seed=42)\n",
    "mask_paths = tf.data.Dataset.list_files(train_mask_dir, seed=42)\n",
    "dataset = tf.data.Dataset.zip((image_paths, mask_paths))\n",
    "dataset = dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.shuffle(1000)\n",
    "\n",
    "# Calculate dataset sizes\n",
    "total_size = dataset.cardinality().numpy()\n",
    "val_size = total_size // 10\n",
    "test_size = total_size // 10\n",
    "train_size = total_size - (val_size + test_size)\n",
    "\n",
    "# Dataset partitioning\n",
    "train_dataset = dataset.take(train_size).batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = dataset.skip(train_size).take(val_size).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = dataset.skip(train_size + val_size).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Prefetching datasets\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Model creation function using the detailed U-Net architecture\n",
    "def get_model(n_classes, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
    "    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n",
    "\n",
    "# Setup mixed precision policy\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Create and compile the model\n",
    "model = get_model(n_classes, IMG_SIZE[0], IMG_SIZE[1], 1)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['Accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=3,\n",
    "    validation_data=val_dataset,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('optimized_model_3epochs_multi_U_net.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with class imbalance and preventing overfitting are two important aspects of training a neural network, especially for tasks like segmentation where certain classes may be underrepresented. Here are strategies to handle both issues effectively:\n",
    "\n",
    "### Handling Class Imbalance\n",
    "\n",
    "1. **Weighted Loss Function:**\n",
    "   You can assign a higher loss weight to underrepresented classes. TensorFlow allows you to use `class_weight` in the `fit` method for classification tasks. For segmentation, you can modify your loss function directly to account for class imbalance by using weights inside the categorical cross-entropy calculation.\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras import backend as K\n",
    "\n",
    "   def weighted_categorical_crossentropy(weights):\n",
    "       def wcce(y_true, y_pred):\n",
    "           Kweights = K.constant(weights)\n",
    "           y_true = K.cast(y_true, y_pred.dtype)\n",
    "           return K.categorical_crossentropy(y_true, y_pred) * K.sum(y_true * Kweights, axis=-1)\n",
    "       return wcce\n",
    "\n",
    "   # Example of weights for three classes where the second class is more rare\n",
    "   weights = [1, 2, 1]\n",
    "   loss = weighted_categorical_crossentropy(weights)\n",
    "   ```\n",
    "\n",
    "2. **Focal Loss:**\n",
    "   This is particularly useful for handling class imbalance in segmentation. It modifies the cross-entropy loss to down-weight easy examples and focus training on hard negatives.\n",
    "\n",
    "   ```python\n",
    "   def focal_loss(gamma=2., alpha=4.):\n",
    "       gamma = float(gamma)\n",
    "       alpha = float(alpha)\n",
    "       def focal_loss_fixed(y_true, y_pred):\n",
    "           epsilon = K.epsilon()\n",
    "           y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "           y_true = K.cast(y_true, y_pred.dtype)\n",
    "           alpha_t = y_true*alpha + (K.ones_like(y_true)-y_true)*(1-alpha)\n",
    "           p_t = y_true*y_pred + (K.ones_like(y_true)-y_true)*(1-y_pred)\n",
    "           fl = - K.log(p_t) * K.pow((K.ones_like(y_true)-p_t), gamma) * alpha_t\n",
    "           return K.mean(fl)\n",
    "       return focal_loss_fixed\n",
    "   ```\n",
    "\n",
    "### Preventing Overfitting\n",
    "\n",
    "1. **Regularization:**\n",
    "   Adding L1 or L2 regularization to the convolutional layers can help reduce overfitting by penalizing large weights.\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.regularizers import l2\n",
    "\n",
    "   # Example adding L2 regularization to a layer\n",
    "   x = Conv2D(filters, (3, 3), padding='same', kernel_regularizer=l2(0.01))(x)\n",
    "   ```\n",
    "\n",
    "2. **Dropout:**\n",
    "   You're already using Dropout, which is great. You might experiment with placing Dropout layers at different points in your network or adjusting the dropout rate.\n",
    "\n",
    "3. **Early Stopping:**\n",
    "   Monitor validation loss and stop training when it starts to increase, even if the training loss continues to decrease.\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "   early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "   ```\n",
    "\n",
    "4. **Data Augmentation:**\n",
    "   Increasing the diversity of your training set can help generalize better. You can use Keras' `ImageDataGenerator` or `tf.image` for real-time data augmentation.\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "   datagen = ImageDataGenerator(\n",
    "       rotation_range=20,\n",
    "       width_shift_range=0.2,\n",
    "       height_shift_range=0.2,\n",
    "       shear_range=0.1,\n",
    "       zoom_range=0.2,\n",
    "       horizontal_flip=True,\n",
    "       fill_mode='nearest'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### Compile and Fit the Model\n",
    "\n",
    "Incorporate these strategies when compiling and fitting your model:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n",
    "\n",
    "# Assuming 'train_data' and 'val_data' are your training and validation datasets\n",
    "model.fit(train_data, epochs=50, validation_data=val_data, callbacks=[early_stopping])\n",
    "```\n",
    "\n",
    "Adjust these strategies based on the specific requirements and data characteristics of your project. Each technique can have a significant impact, so it’s worth experimenting with various combinations to see what works best for your specific scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 23:04:14.926753: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU = 0.01840406\n",
      "[[ 1317112.   795464.  1227881.   944340.]\n",
      " [16777216.   198817.   119481.    50731.]\n",
      " [       0.        0.        0.        0.]\n",
      " [       0.        0.        0.        0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.metrics import MeanIoU\n",
    "\n",
    "# Prepare to collect predictions and actuals\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "for x_batch, y_batch in test_dataset:\n",
    "    y_pred = model.predict(x_batch)\n",
    "    y_pred_argmax = np.argmax(y_pred, axis=3)\n",
    "    y_true_list.append(y_batch.numpy()[:, :, :, 0])\n",
    "    y_pred_list.append(y_pred_argmax)\n",
    "\n",
    "# Convert lists to single numpy arrays\n",
    "y_true = np.concatenate(y_true_list, axis=0)\n",
    "y_pred_argmax = np.concatenate(y_pred_list, axis=0)\n",
    "\n",
    "# Calculate Mean IoU using Keras\n",
    "n_classes = 4\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)\n",
    "IOU_keras.update_state(y_true, y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "confusion_mtx = IOU_keras.total_cm.numpy()  # Accessing the confusion matrix\n",
    "print(confusion_mtx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m test_img_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(test_img[:, :, \u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Ensure dimensions are correct\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Make a prediction\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_img_input)\n\u001b[1;32m     11\u001b[0m predicted_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)[\u001b[38;5;241m0\u001b[39m, :, :]  \u001b[38;5;66;03m# Convert predictions to label format\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Com_Conda/.conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Com_Conda/.conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract a single batch from the dataset\n",
    "for x_batch, y_batch in test_dataset.take(1):\n",
    "    test_img = x_batch[0]  # Take the first image from the batch\n",
    "    ground_truth = y_batch[0]  # Corresponding ground truth\n",
    "    test_img_input = tf.expand_dims(test_img[:, :, 0], axis=0)  # Ensure dimensions are correct\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(test_img_input)\n",
    "    predicted_img = np.argmax(prediction, axis=3)[0, :, :]  # Convert predictions to label format\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title('Testing Image')\n",
    "    plt.imshow(test_img[:, :, 0], cmap='gray')  # Display the first channel\n",
    "    plt.subplot(232)\n",
    "    plt.title('Testing Label')\n",
    "    plt.imshow(ground_truth[:, :, 0], cmap='jet')  # Display the first channel of ground truth\n",
    "    plt.subplot(233)\n",
    "    plt.title('Prediction on test image')\n",
    "    plt.imshow(predicted_img, cmap='jet')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU for class 1 is: 0.06253495812416077\n",
      "IoU for class 2 is: 0.011081274598836899\n",
      "IoU for class 3 is: 0.0\n",
      "IoU for class 4 is: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Class-wise IoU from confusion matrix\n",
    "class_iou = []\n",
    "for i in range(n_classes):\n",
    "    iou = confusion_mtx[i, i] / (np.sum(confusion_mtx[i, :]) + np.sum(confusion_mtx[:, i]) - confusion_mtx[i, i])\n",
    "    class_iou.append(iou)\n",
    "    print(f\"IoU for class {i+1} is: {iou}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def label_to_color_image(label):\n",
    "    \"\"\"Convert a 2D array label to a color image.\n",
    "    \n",
    "    Args:\n",
    "        label: A 2D array with integer type, storing the segmentation label.\n",
    "    \n",
    "    Returns:\n",
    "        result: A 2D array with three channels (RGB), where each element in the label\n",
    "                is mapped to a corresponding RGB color.\n",
    "    \"\"\"\n",
    "    # Define the colormap\n",
    "    color_map = np.array([\n",
    "        [0, 0, 0],        # Class 0 -> Black\n",
    "        [255, 0, 0],      # Class 1 -> Red\n",
    "        [0, 255, 0],      # Class 2 -> Green\n",
    "        [0, 0, 255]       # Class 3 -> Blue\n",
    "    ])\n",
    "\n",
    "    # Map the label to the corresponding color\n",
    "    img = np.take(color_map, label, axis=0)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'predicted_img' is a 2D array with class labels as integers (output from argmax)\n",
    "# predicted_img_color = label_to_color_image(predicted_img)\n",
    "# plt.imshow(predicted_img_color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m y_batch[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Corresponding ground truth\u001b[39;00m\n\u001b[1;32m      4\u001b[0m test_img_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(test_img[:, :, \u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Prepare input\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_img_input)\n\u001b[1;32m      7\u001b[0m predicted_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)[\u001b[38;5;241m0\u001b[39m, :, :]  \u001b[38;5;66;03m# Convert predictions to label format\u001b[39;00m\n\u001b[1;32m      8\u001b[0m predicted_img_color \u001b[38;5;241m=\u001b[39m label_to_color_image(predicted_img)  \u001b[38;5;66;03m# Convert to color\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Com_Conda/.conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Com_Conda/.conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in test_dataset.take(1):\n",
    "    test_img = x_batch[0]  # First image in the batch\n",
    "    ground_truth = y_batch[0]  # Corresponding ground truth\n",
    "    test_img_input = tf.expand_dims(test_img[:, :, 0], axis=0)  # Prepare input\n",
    "\n",
    "    prediction = model.predict(test_img_input)\n",
    "    predicted_img = np.argmax(prediction, axis=3)[0, :, :]  # Convert predictions to label format\n",
    "    predicted_img_color = label_to_color_image(predicted_img)  # Convert to color\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title('Testing Image')\n",
    "    plt.imshow(test_img[:, :, 0], cmap='gray')\n",
    "    plt.subplot(232)\n",
    "    plt.title('Testing Label')\n",
    "    plt.imshow(label_to_color_image(ground_truth[:, :, 0]))  # Display ground truth in color\n",
    "    plt.subplot(233)\n",
    "    plt.title('Prediction on test image')\n",
    "    plt.imshow(predicted_img_color)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".C_Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
